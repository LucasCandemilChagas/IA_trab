{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest para Classificação do Jogo da Velha\n",
    "## Comparação com Decision Tree Classifier\n",
    "\n",
    "Este notebook implementa um modelo Random Forest para classificar estados do jogo da velha e compara com o Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração para melhor visualização\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "velha = pd.read_csv('IA_trab-lucas_branch/amostras_.csv', sep=';')\n",
    "\n",
    "print(f\"Dataset shape: {velha.shape}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "print(velha.head())\n",
    "\n",
    "print(f\"\\nClasses disponíveis: {velha['classe'].unique()}\")\n",
    "print(f\"\\nDistribuição das classes:\")\n",
    "print(velha['classe'].value_counts())\n",
    "\n",
    "# Separar features e target\n",
    "X = velha.drop(columns=['classe'])\n",
    "y = velha['classe']\n",
    "\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")\n",
    "print(f\"Target shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão estratificada dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.1, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"\\nDistribuição no conjunto de teste:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Visualização da distribuição\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "y_train.value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribuição - Conjunto de Treino')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_test.value_counts().plot(kind='bar', color='lightcoral')\n",
    "plt.title('Distribuição - Conjunto de Teste')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Random Forest com parâmetros padrão\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos os cores disponíveis\n",
    ")\n",
    "\n",
    "print(\"Treinando o modelo Random Forest...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Treinamento concluído!\")\n",
    "\n",
    "# Predições\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nAcurácia: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualização da matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, \n",
    "    display_labels=['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']\n",
    ")\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Matriz de Confusão - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']))\n",
    "\n",
    "# Análise detalhada por classe\n",
    "print(\"\\nAnálise Detalhada por Classe:\")\n",
    "classes = ['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']\n",
    "for i, classe in enumerate(classes):\n",
    "    tp = cm[i, i]\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    tn = cm.sum() - tp - fp - fn\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{classe}:\")\n",
    "    print(f\"  Precisão: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(f\"  Recall: {recall:.4f} ({recall*100:.2f}%)\")\n",
    "    print(f\"  F1-Score: {f1:.4f} ({f1*100:.2f}%)\")\n",
    "    print(f\"  Verdadeiros Positivos: {tp}\")\n",
    "    print(f\"  Falsos Positivos: {fp}\")\n",
    "    print(f\"  Falsos Negativos: {fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validação cruzada\n",
    "print(\"Validação Cruzada (5-fold):\")\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Scores de validação cruzada: {cv_scores}\")\n",
    "print(f\"Média: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Visualização dos scores de validação cruzada\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 6), cv_scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Média: {cv_scores.mean():.4f}')\n",
    "plt.fill_between(range(1, 6), \n",
    "                 cv_scores.mean() - cv_scores.std(), \n",
    "                 cv_scores.mean() + cv_scores.std(), \n",
    "                 alpha=0.2, color='red')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.title('Scores de Validação Cruzada')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o grid de parâmetros para otimização\n",
    "import numpy as np\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "print(\"Grid de parâmetros definido:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(f\"\\nTotal de combinações: {np.prod([len(v) for v in param_grid.values()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar GridSearchCV\n",
    "print(\"Iniciando GridSearchCV...\")\n",
    "print(\"Isso pode levar alguns minutos...\")\n",
    "\n",
    "# Garantir que as importações estejam disponíveis\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    print(\"Bibliotecas importadas com sucesso!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Erro na importação: {e}\")\n",
    "\n",
    "# Criar o modelo base\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Usar todos os cores disponíveis\n",
    "    verbose=1  # Mostrar progresso\n",
    ")\n",
    "\n",
    "# Executar a busca\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nGridSearchCV concluído!\")\n",
    "print(f\"Melhor score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Melhores parâmetros: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise dos resultados do GridSearchCV\n",
    "print(\"=== ANÁLISE DOS RESULTADOS ===\")\n",
    "\n",
    "# Top 10 melhores combinações\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_results = results_df.nlargest(10, 'mean_test_score')[\n",
    "    ['params', 'mean_test_score', 'std_test_score']\n",
    "]\n",
    "\n",
    "print(\"\\nTop 10 melhores combinações:\")\n",
    "for i, (_, row) in enumerate(top_results.iterrows(), 1):\n",
    "    print(f\"\\n{i}. Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']*2:.4f})\")\n",
    "    print(f\"   Parâmetros: {row['params']}\")\n",
    "\n",
    "# Comparação com modelo original\n",
    "print(f\"\\n=== COMPARAÇÃO ===\")\n",
    "print(f\"Modelo original (parâmetros padrão):\")\n",
    "print(f\"  Acurácia: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "print(f\"  Parâmetros: n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt'\")\n",
    "\n",
    "print(f\"\\nMelhor modelo encontrado:\")\n",
    "print(f\"  Acurácia: {grid_search.best_score_:.4f} (+/- {grid_search.cv_results_['std_test_score'][grid_search.best_index_]*2:.4f})\")\n",
    "print(f\"  Parâmetros: {grid_search.best_params_}\")\n",
    "\n",
    "improvement = ((grid_search.best_score_ - cv_scores.mean()) / cv_scores.mean()) * 100\n",
    "print(f\"\\nMelhoria: {improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo final com melhores parâmetros\n",
    "print(\"Treinando modelo final com melhores parâmetros...\")\n",
    "\n",
    "# Usar o melhor modelo encontrado\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Fazer predições no conjunto de teste\n",
    "y_pred_best = best_rf_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Acurácia no conjunto de teste: {accuracy_best:.4f} ({accuracy_best*100:.2f}%)\")\n",
    "\n",
    "# Comparação final\n",
    "print(f\"\\n=== COMPARAÇÃO FINAL ===\")\n",
    "print(f\"Modelo original: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Modelo otimizado: {accuracy_best:.4f} ({accuracy_best*100:.2f}%)\")\n",
    "\n",
    "test_improvement = ((accuracy_best - accuracy) / accuracy) * 100\n",
    "print(f\"Melhoria no teste: {test_improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da comparação de performance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Gráfico 1: Comparação de acurácias\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['Original', 'Otimizado']\n",
    "accuracies = [accuracy, accuracy_best]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = plt.bar(models, accuracies, color=colors, alpha=0.8)\n",
    "plt.title('Comparação de Acurácia')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Gráfico 2: Comparação de validação cruzada\n",
    "plt.subplot(1, 2, 2)\n",
    "cv_original = cv_scores.mean()\n",
    "cv_optimized = grid_search.best_score_\n",
    "\n",
    "cv_models = ['Original', 'Otimizado']\n",
    "cv_scores_comp = [cv_original, cv_optimized]\n",
    "cv_colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars2 = plt.bar(cv_models, cv_scores_comp, color=cv_colors, alpha=0.8)\n",
    "plt.title('Validação Cruzada (5-fold)')\n",
    "plt.ylabel('Acurácia Média')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, acc in zip(bars2, cv_scores_comp):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7775ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão do modelo otimizado\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Comparação das matrizes de confusão\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Matriz original\n",
    "disp1 = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, \n",
    "    display_labels=['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']\n",
    ")\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Matriz de Confusão - Modelo Original')\n",
    "\n",
    "# Matriz otimizada\n",
    "disp2 = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_best, \n",
    "    display_labels=['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']\n",
    ")\n",
    "disp2.plot(ax=axes[1], cmap='Greens', values_format='d')\n",
    "axes[1].set_title('Matriz de Confusão - Modelo Otimizado')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Relatório de classificação do modelo otimizado\n",
    "print(\"Relatório de Classificação - Modelo Otimizado:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Em_jogo', 'Possibilidade_de_fim_de_jogo', 'Fim_de_jogo']))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
